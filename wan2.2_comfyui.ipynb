{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Wan2.2 Video Generation on Google Colab\n\nWan2.2をGoogle Colabで動かすためのノートブック\n\n## 特徴\n- **Google Driveにモデル保存** - 毎回ダウンロード不要\n- **複数モデル対応** - 用途に応じて選択可能\n- **GGUF量子化対応** - T4でも14Bモデル動作可能\n- **カスタムノード対応** - WanVideoWrapper, GGUF, VideoHelperSuite\n\n## 対応モデル\n\n### Checkpoints（オールインワン型）\n| モデル | VRAM目安 | 用途 |\n|--------|----------|------|\n| Rapid-AIO v12.2 | ~8GB | 高速生成（4step） |\n\n### Diffusion Models（モジュラー型）\n| モデル | VRAM目安 | 用途 |\n|--------|----------|------|\n| I2V-14B-fp16 (High/Low Noise) | 24GB+ | 高品質I2V |\n| I2V-14B-GGUF-Q4 | ~12GB | T4対応I2V |\n| Animate-14B-GGUF-Q4 | ~12GB | アニメーション |\n| TI2V-5B | ~8GB | 軽量版 |\n| T2V-14B-fp16 | 24GB+ | テキスト→動画 |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. GPU確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "!nvidia-smi\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "    if gpu_memory >= 40:\n",
    "        print(\"推奨: fp16モデル (I2V-14B, T2V-14B)\")\n",
    "    elif gpu_memory >= 20:\n",
    "        print(\"推奨: fp16またはGGUF-Q8\")\n",
    "    else:\n",
    "        print(\"推奨: GGUF-Q4量子化モデル or TI2V-5B\")\n",
    "else:\n",
    "    print(\"GPU not available!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Google Driveマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport os\n\ndrive.mount('/content/drive')\n\n# モデル保存用ディレクトリ\nDRIVE_MODEL_DIR = \"/content/drive/MyDrive/ComfyUI_Wan22\"\n\n# フォルダ構成作成\nfolders = [\n    f\"{DRIVE_MODEL_DIR}/checkpoints\",\n    f\"{DRIVE_MODEL_DIR}/diffusion_models\",\n    f\"{DRIVE_MODEL_DIR}/text_encoders\",\n    f\"{DRIVE_MODEL_DIR}/vae\",\n    f\"{DRIVE_MODEL_DIR}/clip_vision\",\n    f\"{DRIVE_MODEL_DIR}/loras\",\n    f\"{DRIVE_MODEL_DIR}/workflows\",\n    f\"{DRIVE_MODEL_DIR}/outputs\"\n]\nfor folder in folders:\n    os.makedirs(folder, exist_ok=True)\n\nprint(f\"モデル保存先: {DRIVE_MODEL_DIR}\")\n\n# GitHubからワークフローをダウンロード\nGITHUB_RAW = \"https://raw.githubusercontent.com/hatoya0703/Colab.Wan/main/workflows\"\nworkflows = [\n    \"rapid-aio-t2v.json\",\n    \"rapid-aio-i2v.json\"\n]\nprint(\"\\nワークフローをGitHubからダウンロード:\")\nfor wf in workflows:\n    dest = f\"{DRIVE_MODEL_DIR}/workflows/{wf}\"\n    if not os.path.exists(dest):\n        !wget -q -O \"{dest}\" \"{GITHUB_RAW}/{wf}\"\n        print(f\"  DL: {wf}\")\n    else:\n        print(f\"  既存: {wf}\")\n\nprint(\"\\n既存のモデル:\")\n!ls -la \"{DRIVE_MODEL_DIR}/checkpoints/\" 2>/dev/null || echo \"(なし)\"\n!ls -la \"{DRIVE_MODEL_DIR}/diffusion_models/\" 2>/dev/null || echo \"(なし)\"\n!ls -la \"{DRIVE_MODEL_DIR}/text_encoders/\" 2>/dev/null || echo \"(なし)\"\nprint(\"\\n既存のワークフロー:\")\n!ls -la \"{DRIVE_MODEL_DIR}/workflows/\" 2>/dev/null || echo \"(なし)\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 設定パネル { display-mode: \"form\" }\n\n#@markdown ### Checkpoints（オールインワン型）\nDL_RAPID_AIO = False #@param {type:\"boolean\"}\n\n#@markdown ### Diffusionモデル（複数選択可）\nDL_I2V_14B_FP16 = False #@param {type:\"boolean\"}\nDL_I2V_14B_GGUF_Q4 = True #@param {type:\"boolean\"}\nDL_ANIMATE_14B_GGUF_Q4 = False #@param {type:\"boolean\"}\nDL_TI2V_5B = False #@param {type:\"boolean\"}\nDL_T2V_14B_FP16 = False #@param {type:\"boolean\"}\n\n#@markdown ### 共通モデル（Diffusion Models用）\nDL_TEXT_ENCODER = True #@param {type:\"boolean\"}\nDL_VAE = True #@param {type:\"boolean\"}\nDL_CLIP_VISION = True #@param {type:\"boolean\"}\nDL_LORA_ANIMATE = False #@param {type:\"boolean\"}\n\n#@markdown ### トンネル設定\nNGROK_TOKEN = \"\" #@param {type:\"string\"}\n\n# 選択されたモデルを表示\nprint(\"=== ダウンロード対象 ===\")\nselected = []\nif DL_RAPID_AIO: selected.append(\"Rapid-AIO v12.2 (Checkpoint)\")\nif DL_I2V_14B_FP16: selected.append(\"I2V-14B-fp16 (High+Low Noise)\")\nif DL_I2V_14B_GGUF_Q4: selected.append(\"I2V-14B-GGUF-Q4\")\nif DL_ANIMATE_14B_GGUF_Q4: selected.append(\"Animate-14B-GGUF-Q4\")\nif DL_TI2V_5B: selected.append(\"TI2V-5B\")\nif DL_T2V_14B_FP16: selected.append(\"T2V-14B-fp16\")\nif DL_TEXT_ENCODER: selected.append(\"Text Encoder (umt5-xxl-fp8)\")\nif DL_VAE: selected.append(\"VAE\")\nif DL_CLIP_VISION: selected.append(\"CLIP Vision\")\nif DL_LORA_ANIMATE: selected.append(\"LoRA (WanAnimate relight)\")\n\nfor s in selected:\n    print(f\"  - {s}\")\nif not selected:\n    print(\"  (なし - 既存モデルを使用)\")\n\n# Checkpointモデル使用時の注意\nif DL_RAPID_AIO:\n    print(\"\\n⚠️ Rapid-AIOはCheckpoint型です\")\n    print(\"  → Load Checkpointノードで読み込み\")\n    print(\"  → 別途VAE/CLIP不要\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. モデルダウンロード（初回 or 追加時のみ実行）\n",
    "\n",
    "既にGoogle Driveにモデルがある場合はスキップ可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nfrom huggingface_hub import hf_hub_download\nimport os\nimport shutil\n\ndef download_model(repo_id, filename, dest_folder, dest_filename=None):\n    \"\"\"モデルをダウンロードしてGoogle Driveに保存\"\"\"\n    dest_path = f\"{DRIVE_MODEL_DIR}/{dest_folder}/{dest_filename or os.path.basename(filename)}\"\n    if os.path.exists(dest_path):\n        print(f\"  既存: {os.path.basename(dest_path)}\")\n        return\n    print(f\"  DL中: {filename}\")\n    downloaded = hf_hub_download(repo_id=repo_id, filename=filename)\n    shutil.copy(downloaded, dest_path)\n    print(f\"  保存: {dest_path}\")\n\n# === Checkpoints (AIO) ===\nif DL_RAPID_AIO:\n    print(\"\\n=== Checkpoints ===\")\n    print(\"\\n[Rapid-AIO v12.2]\")\n    download_model(\n        \"Phr00t/WAN2.2-14B-Rapid-AllInOne\",\n        \"Mega-v12/wan2.2-rapid-mega-aio-nsfw-v12.2.safetensors\",\n        \"checkpoints\"\n    )\n\n# === Diffusion Models ===\nprint(\"\\n=== Diffusion Models ===\")\n\nif DL_I2V_14B_FP16:\n    print(\"\\n[I2V-14B-fp16]\")\n    download_model(\n        \"Comfy-Org/Wan_2.2_ComfyUI_repackaged\",\n        \"split_files/diffusion_models/wan2.2_i2v_A14B_fp16_high_noise.safetensors\",\n        \"diffusion_models\"\n    )\n    download_model(\n        \"Comfy-Org/Wan_2.2_ComfyUI_repackaged\",\n        \"split_files/diffusion_models/wan2.2_i2v_A14B_fp16_low_noise.safetensors\",\n        \"diffusion_models\"\n    )\n\nif DL_I2V_14B_GGUF_Q4:\n    print(\"\\n[I2V-14B-GGUF-Q4]\")\n    download_model(\n        \"city96/Wan2.1-I2V-14B-480P-GGUF\",\n        \"wan2.1-i2v-14b-480p-Q4_K_S.gguf\",\n        \"diffusion_models\"\n    )\n\nif DL_ANIMATE_14B_GGUF_Q4:\n    print(\"\\n[Animate-14B-GGUF-Q4]\")\n    download_model(\n        \"Kijai/WanVideo_comfy_GGUF\",\n        \"Wan2_2_Animate_14B_Q4_K_M.gguf\",\n        \"diffusion_models\"\n    )\n\nif DL_TI2V_5B:\n    print(\"\\n[TI2V-5B]\")\n    download_model(\n        \"Comfy-Org/Wan_2.2_ComfyUI_repackaged\",\n        \"split_files/diffusion_models/wan2.2_ti2v_5B_fp16.safetensors\",\n        \"diffusion_models\"\n    )\n\nif DL_T2V_14B_FP16:\n    print(\"\\n[T2V-14B-fp16]\")\n    download_model(\n        \"Comfy-Org/Wan_2.2_ComfyUI_repackaged\",\n        \"split_files/diffusion_models/wan2.2_t2v_A14B_fp16.safetensors\",\n        \"diffusion_models\"\n    )\n\n# === Text Encoder ===\nif DL_TEXT_ENCODER:\n    print(\"\\n=== Text Encoder ===\")\n    download_model(\n        \"Kijai/WanVideo_comfy\",\n        \"umt5-xxl-enc-fp8_e4m3fn.safetensors\",\n        \"text_encoders\"\n    )\n\n# === VAE ===\nif DL_VAE:\n    print(\"\\n=== VAE ===\")\n    download_model(\n        \"Kijai/WanVideo_comfy\",\n        \"Wan2_1_VAE_bf16.safetensors\",\n        \"vae\"\n    )\n\n# === CLIP Vision ===\nif DL_CLIP_VISION:\n    print(\"\\n=== CLIP Vision ===\")\n    download_model(\n        \"Comfy-Org/Wan_2.1_ComfyUI_repackaged\",\n        \"split_files/clip_vision/clip_vision_h.safetensors\",\n        \"clip_vision\"\n    )\n\n# === LoRA ===\nif DL_LORA_ANIMATE:\n    print(\"\\n=== LoRA ===\")\n    download_model(\n        \"Kijai/WanVideo_comfy\",\n        \"Wan22_relight/WanAnimate_relight_lora_fp16.safetensors\",\n        \"loras\",\n        \"WanAnimate_relight_lora_fp16.safetensors\"\n    )\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"ダウンロード完了!\")\nprint(\"=\"*50)\nprint(\"\\n保存されたモデル:\")\n!ls -lh \"{DRIVE_MODEL_DIR}/checkpoints/\" 2>/dev/null || echo \"(なし)\"\n!ls -lh \"{DRIVE_MODEL_DIR}/diffusion_models/\"\n!ls -lh \"{DRIVE_MODEL_DIR}/text_encoders/\"\n!ls -lh \"{DRIVE_MODEL_DIR}/vae/\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ComfyUI + カスタムノード インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "# ComfyUI\n",
    "if not os.path.exists(\"/content/ComfyUI\"):\n",
    "    !git clone https://github.com/comfyanonymous/ComfyUI.git /content/ComfyUI\n",
    "%cd /content/ComfyUI\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q huggingface_hub\n",
    "print(\"ComfyUI installed.\")\n",
    "\n",
    "# Kijai WanVideoWrapper\n",
    "if not os.path.exists(\"custom_nodes/ComfyUI-WanVideoWrapper\"):\n",
    "    !git clone https://github.com/kijai/ComfyUI-WanVideoWrapper.git custom_nodes/ComfyUI-WanVideoWrapper\n",
    "    !pip install -q -r custom_nodes/ComfyUI-WanVideoWrapper/requirements.txt\n",
    "print(\"WanVideoWrapper installed.\")\n",
    "\n",
    "# GGUF support\n",
    "if not os.path.exists(\"custom_nodes/ComfyUI-GGUF\"):\n",
    "    !git clone https://github.com/city96/ComfyUI-GGUF.git custom_nodes/ComfyUI-GGUF\n",
    "    !pip install -q gguf\n",
    "print(\"GGUF support installed.\")\n",
    "\n",
    "# VideoHelperSuite\n",
    "if not os.path.exists(\"custom_nodes/ComfyUI-VideoHelperSuite\"):\n",
    "    !git clone https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite.git custom_nodes/ComfyUI-VideoHelperSuite\n",
    "    !pip install -q -r custom_nodes/ComfyUI-VideoHelperSuite/requirements.txt 2>/dev/null || true\n",
    "print(\"VideoHelperSuite installed.\")\n",
    "\n",
    "print(\"\\nAll custom nodes installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. モデルをシンボリックリンクで接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nos.chdir(\"/content/ComfyUI\")\n\n# 既存のmodelsフォルダをバックアップして削除\n!rm -rf models/checkpoints models/diffusion_models models/text_encoders models/vae models/clip_vision models/loras 2>/dev/null\n\n# シンボリックリンク作成（モデル）\nlinks = [\n    (f\"{DRIVE_MODEL_DIR}/checkpoints\", \"models/checkpoints\"),\n    (f\"{DRIVE_MODEL_DIR}/diffusion_models\", \"models/diffusion_models\"),\n    (f\"{DRIVE_MODEL_DIR}/text_encoders\", \"models/text_encoders\"),\n    (f\"{DRIVE_MODEL_DIR}/vae\", \"models/vae\"),\n    (f\"{DRIVE_MODEL_DIR}/clip_vision\", \"models/clip_vision\"),\n    (f\"{DRIVE_MODEL_DIR}/loras\", \"models/loras\"),\n]\n\nfor src, dst in links:\n    if os.path.exists(src):\n        !ln -sf \"{src}\" \"{dst}\"\n        print(f\"リンク: {dst} -> {src}\")\n\n# ワークフローのシンボリックリンク\nos.makedirs(\"user/default\", exist_ok=True)\n!rm -rf user/default/workflows 2>/dev/null\nif os.path.exists(f\"{DRIVE_MODEL_DIR}/workflows\"):\n    !ln -sf \"{DRIVE_MODEL_DIR}/workflows\" \"user/default/workflows\"\n    print(f\"リンク: user/default/workflows -> {DRIVE_MODEL_DIR}/workflows\")\n\nprint(\"\\n=== モデル確認 ===\")\n!ls -la models/checkpoints/ 2>/dev/null || echo \"checkpoints: (なし)\"\n!ls -la models/diffusion_models/\n!ls -la models/text_encoders/\n!ls -la models/vae/\nprint(\"\\n=== ワークフロー確認 ===\")\n!ls -la user/default/workflows/ 2>/dev/null || echo \"workflows: (なし)\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ComfyUI起動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/ComfyUI\")\n",
    "\n",
    "# ngrokトンネル\n",
    "if NGROK_TOKEN:\n",
    "    !pip install -q pyngrok\n",
    "    from pyngrok import ngrok\n",
    "    ngrok.set_auth_token(NGROK_TOKEN)\n",
    "    public_url = ngrok.connect(8188)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ComfyUI URL: {public_url}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "else:\n",
    "    print(\"=\"*50)\n",
    "    print(\"ngrok未設定\")\n",
    "    print(\"Colabの出力ポートからアクセスしてください\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# ComfyUI起動\n",
    "!python main.py --listen 0.0.0.0 --port 8188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Tips\n\n### 2回目以降の起動\n1. セル1 (GPU確認)\n2. セル2 (Google Driveマウント)\n3. セル5 (ComfyUIインストール)\n4. セル6 (シンボリックリンク)\n5. セル7 (ComfyUI起動)\n\n→ セル3,4はスキップ可能\n\n### GPU別推奨モデル\n| GPU | VRAM | 推奨モデル |\n|-----|------|------------|\n| T4 | 15GB | Rapid-AIO, GGUF-Q4, TI2V-5B |\n| L4 | 24GB | fp16, GGUF-Q8 |\n| A100 | 40GB | fp16 |\n\n### モデルタイプの違い\n| タイプ | フォルダ | 読み込みノード | 備考 |\n|--------|----------|----------------|------|\n| Checkpoint | checkpoints/ | Load Checkpoint | VAE/CLIP内蔵 |\n| Diffusion | diffusion_models/ | WanVideoWrapper等 | 別途VAE/CLIP必要 |\n\n### フォルダ構成\n```\nGoogle Drive/MyDrive/ComfyUI_Wan22/\n├── checkpoints/       # オールインワンモデル\n├── diffusion_models/  # メインモデル（モジュラー型）\n├── text_encoders/     # テキストエンコーダー\n├── vae/               # VAE\n├── clip_vision/       # CLIP Vision\n├── loras/             # LoRA\n├── workflows/         # ワークフロー（自動読み込み）\n└── outputs/           # 出力動画\n```\n\n### Rapid-AIO推奨設定\n- Sampler: `dpmpp_sde`\n- Scheduler: `beta`\n- CFG: `1`\n- Steps: `4`"
  }
 ]
}