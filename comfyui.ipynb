{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ComfyUI on Google Colab\n\nComfyUIをGoogle Colabで動かすためのノートブック\n\n## 特徴\n- **Google Driveにモデル保存** - 毎回ダウンロード不要\n- **複数モデル対応** - 用途に応じて選択可能\n- **GGUF量子化対応** - T4でも14Bモデル動作可能\n- **日本語プロンプト対応** - DeepTranslatorで自動翻訳（Google/DeepL対応）\n- **カスタムノード対応** - WanVideoWrapper, GGUF, VideoHelperSuite, QwenEditUtils, AlekPet, pysssss, LayerStyle, TeaCache\n\n## 対応モデル\n\n### Wan2.2 Diffusion Models（モジュラー型・動画生成）\n| モデル | サイズ | VRAM目安 | 用途 |\n|--------|--------|----------|------|\n| I2V-14B-fp16 | 28.6GB | 24GB+ | 高品質I2V |\n| I2V-14B-fp8 | 14.3GB | ~16GB | 中VRAM I2V |\n| I2V-14B-GGUF-Q4 | ~8GB | ~12GB | T4対応I2V |\n| T2V-14B-fp16 | 28.6GB | 24GB+ | 高品質T2V |\n| T2V-14B-fp8 | 14.3GB | ~16GB | 中VRAM T2V |\n| TI2V-5B | ~10GB | ~8GB | 軽量版（テキスト+画像） |\n| Animate-14B-GGUF-Q4 | ~8GB | ~12GB | アニメーション |\n\n### Qwen Image Edit（チェックポイント型・画像編集）\n| モデル | サイズ | VRAM目安 | 用途 |\n|--------|--------|----------|------|\n| Qwen-Rapid-AIO-NSFW-v18 | 28.4GB | 24GB+ | 画像編集・生成 |\n\n### High Noise vs Low Noise (Wan)\n- **High Noise**: 大きな動き・変化を生成（推奨）\n- **Low Noise**: 入力により忠実、変化控えめ\n\n## ワークフロー\n| ファイル | 用途 | 必要モデル |\n|----------|------|------------|\n| t2v-14b.json | テキスト→動画 | T2V-14B + TextEncoder + VAE |\n| i2v-14b.json | 画像→動画 | I2V-14B + TextEncoder + VAE + CLIP Vision |\n| low-vram-ti2v.json | T4対応TI2V | TI2V-5B + LoRA + TextEncoder + VAE |\n| qwen-image-edit.json | 画像編集 | Qwen-Rapid-AIO-v18 |\n\n## 日本語プロンプト\nDeepTranslatorTextNodeを使用して日本語→英語に自動翻訳できます。\n```\n[DeepTranslatorTextNode] → STRING → [CLIPTextEncode] → モデル\n     日本語入力              翻訳済み英語\n```\n対応サービス: Google翻訳, DeepL, Bing, Yandex等"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. GPU確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "!nvidia-smi\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "    if gpu_memory >= 40:\n",
    "        print(\"推奨: fp16モデル (I2V-14B, T2V-14B)\")\n",
    "    elif gpu_memory >= 20:\n",
    "        print(\"推奨: fp16またはGGUF-Q8\")\n",
    "    else:\n",
    "        print(\"推奨: GGUF-Q4量子化モデル or TI2V-5B\")\n",
    "else:\n",
    "    print(\"GPU not available!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Google Driveマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\nimport os\n\ndrive.mount('/content/drive')\n\n# モデル保存用ディレクトリ\nDRIVE_MODEL_DIR = \"/content/drive/MyDrive/ComfyUI\"\n\n# フォルダ構成作成\nfolders = [\n    f\"{DRIVE_MODEL_DIR}/diffusion_models\",\n    f\"{DRIVE_MODEL_DIR}/text_encoders\",\n    f\"{DRIVE_MODEL_DIR}/vae\",\n    f\"{DRIVE_MODEL_DIR}/clip_vision\",\n    f\"{DRIVE_MODEL_DIR}/loras\",\n    f\"{DRIVE_MODEL_DIR}/checkpoints\",\n    f\"{DRIVE_MODEL_DIR}/workflows\",\n    f\"{DRIVE_MODEL_DIR}/outputs\"\n]\nfor folder in folders:\n    os.makedirs(folder, exist_ok=True)\n\nprint(f\"モデル保存先: {DRIVE_MODEL_DIR}\")\n\n# GitHubからワークフローをダウンロード（常に最新版で上書き）\nGITHUB_RAW = \"https://raw.githubusercontent.com/hatoya0703/Colab.ComfyUI/main/workflows\"\nworkflows = [\n    \"t2v-14b.json\",\n    \"i2v-14b.json\",\n    \"low-vram-ti2v.json\",\n    \"qwen-image-edit.json\"\n]\nprint(\"\\nワークフローをGitHubから取得（常に最新版）:\")\nfor wf in workflows:\n    dest = f\"{DRIVE_MODEL_DIR}/workflows/{wf}\"\n    !wget -q -O \"{dest}\" \"{GITHUB_RAW}/{wf}\" && echo \"  更新: {wf}\" || echo \"  失敗: {wf}\"\n\nprint(\"\\n既存のモデル:\")\n!ls -la \"{DRIVE_MODEL_DIR}/diffusion_models/\" 2>/dev/null || echo \"(なし)\"\n!ls -la \"{DRIVE_MODEL_DIR}/text_encoders/\" 2>/dev/null || echo \"(なし)\"\n!ls -la \"{DRIVE_MODEL_DIR}/checkpoints/\" 2>/dev/null || echo \"(なし)\"\nprint(\"\\n既存のワークフロー:\")\n!ls -la \"{DRIVE_MODEL_DIR}/workflows/\" 2>/dev/null || echo \"(なし)\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. ComfyUI + カスタムノード インストール"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nimport os\n\n# ComfyUI\nif not os.path.exists(\"/content/ComfyUI\"):\n    !git clone https://github.com/comfyanonymous/ComfyUI.git /content/ComfyUI\n%cd /content/ComfyUI\n!pip install -q -r requirements.txt\n!pip install -q huggingface_hub\nprint(\"ComfyUI installed.\")\n\n# Kijai WanVideoWrapper\nif not os.path.exists(\"custom_nodes/ComfyUI-WanVideoWrapper\"):\n    !git clone https://github.com/kijai/ComfyUI-WanVideoWrapper.git custom_nodes/ComfyUI-WanVideoWrapper\n    !pip install -q -r custom_nodes/ComfyUI-WanVideoWrapper/requirements.txt\nprint(\"WanVideoWrapper installed.\")\n\n# GGUF support\nif not os.path.exists(\"custom_nodes/ComfyUI-GGUF\"):\n    !git clone https://github.com/city96/ComfyUI-GGUF.git custom_nodes/ComfyUI-GGUF\n    !pip install -q gguf\nprint(\"GGUF support installed.\")\n\n# VideoHelperSuite\nif not os.path.exists(\"custom_nodes/ComfyUI-VideoHelperSuite\"):\n    !git clone https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite.git custom_nodes/ComfyUI-VideoHelperSuite\n    !pip install -q -r custom_nodes/ComfyUI-VideoHelperSuite/requirements.txt 2>/dev/null || true\nprint(\"VideoHelperSuite installed.\")\n\n# KJNodes (for Low-VRAM TI2V workflow)\nif not os.path.exists(\"custom_nodes/ComfyUI-KJNodes\"):\n    !git clone https://github.com/kijai/ComfyUI-KJNodes.git custom_nodes/ComfyUI-KJNodes\n    !pip install -q -r custom_nodes/ComfyUI-KJNodes/requirements.txt 2>/dev/null || true\nprint(\"KJNodes installed.\")\n\n# rgthree-comfy (for Low-VRAM TI2V workflow)\nif not os.path.exists(\"custom_nodes/rgthree-comfy\"):\n    !git clone https://github.com/rgthree/rgthree-comfy.git custom_nodes/rgthree-comfy\nprint(\"rgthree-comfy installed.\")\n\n# Qwen Image Edit Utils (for Qwen Image Edit workflow)\nif not os.path.exists(\"custom_nodes/Comfyui-QwenEditUtils\"):\n    !git clone https://github.com/lrzjason/Comfyui-QwenEditUtils.git custom_nodes/Comfyui-QwenEditUtils\nprint(\"QwenEditUtils installed.\")\n\n# AlekPet Custom Nodes (日本語→英語翻訳: DeepTranslator)\nif not os.path.exists(\"custom_nodes/ComfyUI_Custom_Nodes_AlekPet\"):\n    !git clone https://github.com/AlekPet/ComfyUI_Custom_Nodes_AlekPet.git custom_nodes/ComfyUI_Custom_Nodes_AlekPet\n    !pip install -q deep-translator\nprint(\"AlekPet Nodes (DeepTranslator) installed.\")\n\n# ComfyUI-Custom-Scripts (pysssss - ShowText node)\nif not os.path.exists(\"custom_nodes/ComfyUI-Custom-Scripts\"):\n    !git clone https://github.com/pythongosssss/ComfyUI-Custom-Scripts.git custom_nodes/ComfyUI-Custom-Scripts\nprint(\"Custom-Scripts (pysssss) installed.\")\n\n# ComfyUI_LayerStyle (for LayerUtility: PurgeVRAM V2)\nif not os.path.exists(\"custom_nodes/ComfyUI_LayerStyle\"):\n    !git clone https://github.com/chflame163/ComfyUI_LayerStyle.git custom_nodes/ComfyUI_LayerStyle\n    !pip install -q -r custom_nodes/ComfyUI_LayerStyle/requirements.txt 2>/dev/null || true\nprint(\"LayerStyle (PurgeVRAM V2) installed.\")\n\n# ComfyUI-TeaCache (for TeaCache optimization)\nif not os.path.exists(\"custom_nodes/ComfyUI-TeaCache\"):\n    !git clone https://github.com/welltop-cn/ComfyUI-TeaCache.git custom_nodes/ComfyUI-TeaCache\nprint(\"TeaCache installed.\")\n\n# SageAttention (for Low-VRAM optimization)\n!pip install -q sageattention\nprint(\"SageAttention installed.\")\n\nprint(\"\\nAll custom nodes installed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. モデルをシンボリックリンクで接続"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nos.chdir(\"/content/ComfyUI\")\n\n# 既存のmodelsフォルダを削除\n!rm -rf models/diffusion_models models/text_encoders models/vae models/clip_vision models/loras models/checkpoints 2>/dev/null\n\n# シンボリックリンク作成（モデル）\nlinks = [\n    (f\"{DRIVE_MODEL_DIR}/diffusion_models\", \"models/diffusion_models\"),\n    (f\"{DRIVE_MODEL_DIR}/text_encoders\", \"models/text_encoders\"),\n    (f\"{DRIVE_MODEL_DIR}/vae\", \"models/vae\"),\n    (f\"{DRIVE_MODEL_DIR}/clip_vision\", \"models/clip_vision\"),\n    (f\"{DRIVE_MODEL_DIR}/loras\", \"models/loras\"),\n    (f\"{DRIVE_MODEL_DIR}/checkpoints\", \"models/checkpoints\"),\n]\n\nfor src, dst in links:\n    if os.path.exists(src):\n        !ln -sf \"{src}\" \"{dst}\"\n        print(f\"リンク: {dst} -> {src}\")\n\n# ワークフローのシンボリックリンク\nos.makedirs(\"user/default\", exist_ok=True)\n!rm -rf user/default/workflows 2>/dev/null\nif os.path.exists(f\"{DRIVE_MODEL_DIR}/workflows\"):\n    !ln -sf \"{DRIVE_MODEL_DIR}/workflows\" \"user/default/workflows\"\n    print(f\"リンク: user/default/workflows -> {DRIVE_MODEL_DIR}/workflows\")\n\nprint(\"\\n=== モデル確認 ===\")\n!ls -la models/diffusion_models/\n!ls -la models/text_encoders/\n!ls -la models/vae/\n!ls -la models/clip_vision/\n!ls -la models/checkpoints/\nprint(\"\\n=== ワークフロー確認 ===\")\n!ls -la user/default/workflows/ 2>/dev/null || echo \"workflows: (なし)\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. ComfyUI起動"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.chdir(\"/content/ComfyUI\")\n\n# URLを取得（ngrok優先、失敗時はColab標準）\ncomfyui_url = None\n\n# 1. ngrokを試す\nfrom google.colab import userdata\ntry:\n    NGROK_TOKEN = userdata.get('NGROK_TOKEN')\n    if NGROK_TOKEN:\n        !pip install -q pyngrok\n        from pyngrok import ngrok\n        ngrok.kill()  # 既存のトンネルを停止\n        ngrok.set_auth_token(NGROK_TOKEN)\n        try:\n            public_url = ngrok.connect(8188)\n            comfyui_url = str(public_url)\n            print(f\"[ngrok] 接続成功\")\n        except Exception as e:\n            print(f\"[ngrok] 接続失敗: {e}\")\nexcept userdata.SecretNotFoundError:\n    print(\"[ngrok] NGROK_TOKEN未設定\")\nexcept Exception as e:\n    print(f\"[ngrok] エラー: {e}\")\n\n# 2. ngrok失敗時はColab標準を使用\nif comfyui_url is None:\n    from google.colab.output import eval_js\n    comfyui_url = eval_js(\"google.colab.kernel.proxyPort(8188)\")\n    print(f\"[Colab] ポート転送を使用\")\n\nprint(f\"\\n{'='*50}\")\nprint(f\"ComfyUI URL: {comfyui_url}\")\nprint(f\"{'='*50}\\n\")\n\n# ComfyUI起動\n!python main.py --listen 0.0.0.0 --port 8188"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Tips\n\n### 起動手順\n1. GPU確認\n2. Google Driveマウント\n3. ComfyUIインストール\n4. シンボリックリンク\n5. ComfyUI起動\n\n### GPU別推奨モデル\n| GPU | VRAM | 推奨モデル |\n|-----|------|------------|\n| T4 | 15GB | GGUF-Q4, TI2V-5B, fp8 |\n| L4 | 24GB | fp16, fp8, Qwen |\n| A100 | 40GB | fp16, Qwen |\n\n### フォルダ構成\n```\nGoogle Drive/MyDrive/ComfyUI/\n├── diffusion_models/  # Wan2.2モデル（モジュラー型）\n├── text_encoders/     # テキストエンコーダー\n├── vae/               # VAE\n├── clip_vision/       # CLIP Vision\n├── loras/             # LoRA\n├── checkpoints/       # Qwenモデル（チェックポイント型）\n├── workflows/         # ワークフロー（自動読み込み）\n└── outputs/           # 出力動画・画像\n```\n\n### Wan 14B推奨設定\n- Sampler: `euler`\n- Scheduler: `simple`\n- CFG: `5`\n- Steps: `30`\n- ModelSamplingSD3 shift: `8`\n\n### Qwen Image Edit v18推奨設定\n- Sampler: `euler_ancestral`\n- Scheduler: `beta`\n- CFG: `1`\n- Steps: `4`"
  }
 ]
}